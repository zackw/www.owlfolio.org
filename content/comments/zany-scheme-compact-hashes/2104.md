slug:    2104
date:    2011-06-26 14:20:26
author:  tom jones
email:   hJy0DnuhxL6_.bZwnIb3kWGNIgfDz.uiaC5NqXCvaIyZZHew==
replyto: 2091

"The people designing cryptographic hashes have gone to considerable
trouble to ensure that two different documents are overwhelmingly
likely to have different fingerprints."

IMHO, this statement is "factually incorrect". i have researched this,
both previously, and now for this conversation, and i am yet to find
any sentence that supports this claim. i know that "not finding
something" is not equivalent as "proof of non-existence", and i know i
am about to quote wikipedia, but still.. (OTOH, it would take for you
to cite just one paper to disprove my reasoning, so i am ok with
shifting the burden to you -- a security researcher ;)

except for the avalanche effect (which only applies to <i>similar</i>
documents), the only thing that ensures two different (unrelated)
documents are likely to have different hashes is the size of the hash
key, <b>and nothing else</b>. all the requirements for cryptographic
hash functions ensure that it is difficult to <b>find</b> two
documents that hash to the same fingerprint, not that there are not
any (or that they are unlikely): <a
href="http://en.wikipedia.org/wiki/Cryptographic_hash_function"
rel="nofollow">WP:Cryptographic_hash_function</a>

<blockquote> * it is easy (but not necessarily quick) to compute the hash value for any given message,
 * it is infeasible to generate a message that has a given hash,
 * it is infeasible to modify a message without hash being changed,
 * it is infeasible to find two different messages with the same hash.</blockquote>

to contrast, the best that any hash function can hope for (except for
perfect hash functions which is a different thing) is to be as good
(unique) as a random sequence of bits: <a
href="http://en.wikipedia.org/wiki/Hash_function#Universal_hashing"
rel="nofollow">WP:Hash_function#Universal_hashing</a> (emphasis mine)

<blockquote>A universal hashing scheme is a randomized algorithm that
selects a hashing function h among a family of such functions, in such
a way that the probability of a collision of any two distinct keys is
1/n, where n is the number of distinct hash values
desiredâ€”independently of the two keys. Universal hashing ensures (in a
<b>probabilistic</b> sense) that the hash function application will
behave as well as if it were using a random function, for any
distribution of the input data. </blockquote>


to get back to my original point, IMHO, using an alphabet with 20-30%
characters that can be confused with another is equivalent to using
256bit vs 512bit SHA2, meaning that you don't need longer 512bit hash
to ensure uniqueness (or to visually distinguish between
keys/documents), you need more bits to prevent brute force attacks (or
even attacks that exploit a weakness in the algorithm).
